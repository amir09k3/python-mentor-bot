# src/utils/ai_engine.py
import subprocess
import sys

def query_llama3(prompt: str, max_tokens: int = 500) -> str:
    """
    Ø§Ø±Ø³Ø§Ù„ Ø³ÙˆØ§Ù„ Ø¨Ù‡ Llama 3.1 Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù‡Ù…Ù‡ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ollama
    """
    try:
        # ØªÙ‚ÙˆÛŒØª prompt Ø¨Ø±Ø§ÛŒ ØªÙ…Ø±Ú©Ø² Ø±ÙˆÛŒ Ù¾Ø§ÛŒØªÙˆÙ†
        full_prompt = (
            "Ø´Ù…Ø§ ÛŒÚ© Ø§Ø³ØªØ§Ø¯ Ù…Ø§Ù‡Ø± Ù¾Ø§ÛŒØªÙˆÙ† Ù‡Ø³ØªÛŒØ¯. ÙÙ‚Ø· Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù¾Ø§ÛŒØªÙˆÙ† Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯. "
            "Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø§ÛŒÙ† Ø­ÙˆØ²Ù‡ Ø¨ÙˆØ¯ØŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯: Â«Ù…Ù† ÙÙ‚Ø· Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø§ÛŒØªÙˆÙ† Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù….Â»\n\n"
            f"Ø³ÙˆØ§Ù„: {prompt}\n\nÙ¾Ø§Ø³Ø®:"
        )
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² stdin Ø¨Ù‡ Ø¬Ø§ÛŒ --prompt (Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ù‡Ù…Ù‡ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§)
        result = subprocess.run(
            ["ollama", "run", "llama3.1:8b-instruct-q4_0"],
            input=full_prompt,
            capture_output=True,
            text=True,
            timeout=30,
            encoding='utf-8',
            creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == "win32" else 0
        )
        
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            stderr_msg = result.stderr.strip()
            if "model 'llama3.1' not found" in stderr_msg:
                return "âŒ Ù…Ø¯Ù„ llama3.1 Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡. Ù„Ø·ÙØ§Ù‹ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯: ollama pull llama3.1"
            return f"âš ï¸ Ø®Ø·Ø§: {stderr_msg[:200]}"
            
    except subprocess.TimeoutExpired:
        return "â° Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø¨ÙˆØ¯. Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯."
    except FileNotFoundError:
        return "âŒ Ollama Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø² https://ollama.com Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯."
    except Exception as e:
        return f"ğŸ’¥ Ø®Ø·Ø§: {type(e).name}: {str(e)}"